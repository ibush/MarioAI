\documentclass[12pt]{article}

\usepackage{amsmath, graphicx}
\usepackage[backend=bibtex]{biblatex}
\addbibresource{cs221proj.bib}

\begin{document}
\nocite{*}

\title{Project Proposal - Deep Reinforcement on Super Mario Bros}

\author{
  Matthew Chen
  \and
  Isabel Frances Bush
}
\date{}
\maketitle


\section{Abstract}

For this project we will be implementing various reinforcement algorithms to create learning agents to effectively play a variant of the classic Super Mario Bros game. The game is challenging form a AI standpoint as the environment is stochastic as the levels are randomly generated and the state space is large. To address these challenges we will use Q-learning with various Q function approximations from simplistic feature extractors to deep learning implementations.

\section{Task Definition}

The game is defined by a simulator that receives actions from our controller and outputs various environment attributes. Our controller will extract a state from the environmental attributes and rewards. The rewards will be a combination of metrics such as positive rewards from coins collected, monsters destroyed, level completion and negative rewards for collision and death.

\begin{itemize}
\item[] \textbf{Actions} - \{ Left, Right, Jump, Crouch, Run/Fire\}
\item[] \textbf{State}
\begin{enumerate}
\item[] Status - Running, Win, Dead
\item[] Mode - Small, Large, Fire
\item[] Ground - True, False
\item[] isAbletoJump - True, False
\item[] hasShell? - True, False
\item[] ableToShoot - True, False
\item[] CreaturePositions - Array of [x,y] coordinates
\item[] Obstacles - 22x22 array of obstacle type
\end{enumerate}
\end{itemize}

\section{Challenges}

The main challenge is to find a optimal policy for Mario to follow given that the map of the environment is not known in advance. The actions which are taken will lead to rewards in future time steps. Additionally we do not know the transition function thus it would be difficult to use offline methods. We would also like our algorithm to run in real time which means we need to complete all calculations in at most 42 ms. It is not apparent what the optimal feature extraction function is for this specific problem and this we will experiment with hand crafted feature selection as well as deep learning approaches.


\section{Benchmarking}

Our baseline method will be a random action method in which Mario picks a random action with a forward moving bias. We chose this as our baseline method as the controller makes moves regardless of state. We use as our oracle the winning scores for the 2009 Mario AI competition \cite{karakovskiy2012mario}. The difference between the two bounds should provide a good prospective on the improvement that can be made. The base metric that we will use to start off is the distance the agent was able to progress at a given level summed across all levels. Our baseline agent averaged at 13,713 while the score for our oracle was 17,264.

\printbibliography

\end{document}