\documentclass[12pt]{article}

\usepackage{amsmath, graphicx}
\usepackage[margin=1in]{geometry}

\begin{document}
\nocite{*}

\title{Project Proposal - Deep Reinforcement Learning on Super Mario Bros}

\author{
  Matthew Chen
  \and
  Isabel Bush
}
\date{}
\maketitle


\section{Abstract}

For this project we will be implementing various reinforcement learning algorithms to create learning agents to effectively play a variant of the classic Super Mario Bros game. The game is challenging from an AI standpoint as the environment is stochastic with randomly generated levels, and the state space is large. To address these challenges we will use Q-learning with various Q-function approximations ranging from simplistic feature extractors to deep learning implementations.


\section{Task Definition}

The game is defined by a simulator that receives actions from our controller and outputs various environment attributes. The goal of an agent is to maximize the fitness score from taking actions and receiving feedback from the simulator. The fitness score will be defined as a weighted combination of certain states (collecting coins, destroying monsters, etc.) which will accumulate throughout the round.

%We will experiment with different types of feedback the simulator will provide the agent with respect to rewards. One way is an incremental approach where the agent accumulates rewards from multiple states in the game (collecting coins, destroying monsters, etc.). Another approach is to only provide reward feedback at the end of the game in the form of a final fitness score. 

In addition to the randomly generated levels, the simulator also provides up to 40 different difficultly levels. We may evaluate our controller by running it on multiple game levels and determining it's overall fitness score as well as a simple distance travelled metric to compare with our benchmarks, described in a later section.

\section{Previous Work}

The Mario AI competition ran for a few years, but few submissions used reinforcement learning. The winning controllers used A* search since the initial metric only took into account distance travelled and the controllers were not required to learn from scratch. Next-highest ranking controllers used rule-based heuristics \cite{karakovskiy}. In the years since competition, a group has implemented a reinforcement learning algorithm that managed to outperform the rule-based controllers, but still fell short of the A* scores \cite{tsay}. We hope to explore different state and feature definitions than were used in that study, and to extend the algorithms to use deep learning as was done for the Atari games \cite{mnih}.


%\begin{itemize}
%\item[] \textbf{Actions}: \{ Left, Right, Jump, Crouch, Run/Fire \}
%\item[] \textbf{State}:
%\itemsep0em
%\begin{enumerate}
%\item[] Status - Running, Won, Dead
%\item[] Mode - Small, Large, Fire
%\item[] onGround - True, False
%\item[] isAbletoJump - True, False
%\item[] hasShell - True, False
%\item[] ableToShoot - True, False
%\item[] CreaturePositions - Array of [x,y] coordinates
%\item[] Obstacles - 22x22 array of obstacle type
%\end{enumerate}
%\end{itemize}

\section{Challenges}

The main challenge is to find an optimal policy for the agent to follow given that the map of the environment is not known in advance, and actions have a delayed feedback. Since the state transition function is unknown to the controller, it would be difficult to use offline methods -- the controller must learn as it plays. 

Another challenge is creating an algorithm to run in real time, defined by at most 42 ms per frame. Finally, it is not apparent what the optimal feature extraction function is for this specific problem and thus we will experiment with hand-crafted feature selection as well as deep learning techniques.

\section{Approaches}

To address the aforementioned challenges, we will implement reinforcement learning algorithms, which are well-suited to Markov Decision Processes with unknown transition probabilities. We will use Q-learning so that we may calculate the optimal Q-value (and thus optimal policy) directly, without needing to first calculate estimations of transitions and rewards. 

Since the state-space is large and our controller is unlikely to experience all possible states, we will use function approximation to generalize to unseen states. We will begin by defining our own features based on simplifications and combinations of the provided environment attributes, and then we will explore learning the feature vector as well through a neural network representation.

\section{Benchmarking}

Our baseline method will be a random policy in which Mario picks a random action with a forward moving bias. We chose this as our baseline method since the controller chooses actions regardless of state or reward. We use as our oracle the winning scores for the 2009 Mario AI competition \cite{karakovskiy}. The difference between the two bounds should provide a good prospective on the improvement that can be made with our learning controller. The base metric that we will use to start off is the distance the agent was able to progress at a given level summed across all levels. Our baseline agent averaged 13,713 while the score for our oracle was 46,565.


\begin{thebibliography}{1}

\bibitem{karakovskiy} S. Karakovskiy, J. Togelius {\em The Mario AI Benchmark and Competitions} 2012.

\bibitem{tsay}  J. Tsay, C. Chen, J. Hsu {\em Evolving Intelligent Mario Controller by Reinforcement Learning} 2011.

\bibitem{mnih} V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller {\em Playing Atari with Deep Reinforcement Learning} 2013.

\end{thebibliography}

\end{document}